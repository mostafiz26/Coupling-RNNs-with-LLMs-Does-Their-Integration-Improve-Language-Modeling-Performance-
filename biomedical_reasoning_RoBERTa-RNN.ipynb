{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a6244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RoBERTa-RNN using NCBI dataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"ncbi/ncbi_disease\")\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name, add_prefix_space=True) \n",
    "encoder_model = RobertaModel.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Define the custom model\n",
    "class RoBERTaWithBiLSTM(nn.Module):\n",
    "    def __init__(self, encoder_model, hidden_dim, num_labels):\n",
    "        super(RoBERTaWithBiLSTM, self).__init__()\n",
    "        self.encoder = encoder_model\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=encoder_model.config.hidden_size,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_labels)  # Multiply hidden_dim by 2 for bidirectional LSTM\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_states = encoder_outputs.last_hidden_state\n",
    "        # Get the last hidden state of BiLSTM for each sequence in the batch\n",
    "        output, (hn, cn) = self.bilstm(last_hidden_states) \n",
    "        # Concatenate the forward and backward hidden states\n",
    "        logits = self.classifier(torch.cat((hn[-2], hn[-1]), dim=1))\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "num_labels = 3  # Assuming binary classification (defect or no defect)\n",
    "hidden_dim = 512  # Example hidden dimension for BiLSTM\n",
    "model = RoBERTaWithBiLSTM(encoder_model, hidden_dim, num_labels)\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    # Ensure inputs are always lists\n",
    "    inputs = [x if isinstance(x, list) else [x] for x in examples['tokens']] \n",
    "    labels = examples['ner_tags']\n",
    "\n",
    "    # Tokenize the inputs with padding and truncation\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length', is_split_into_words=True)\n",
    "\n",
    "    all_new_labels = []\n",
    "    for i in range(len(examples['tokens'])):\n",
    "        word_ids = model_inputs.word_ids(batch_index=i)  # Get word IDs for each example\n",
    "        new_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                new_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                new_labels.append(labels[i][word_idx])  # Access labels for the current example using index 'i'\n",
    "            else:\n",
    "                new_labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_new_labels.append(new_labels)\n",
    "\n",
    "    model_inputs[\"labels\"] = all_new_labels\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',    \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = [l for label_list in labels for l in label_list if l != -100]\n",
    "    true_predictions = []\n",
    "    for i, label_list in enumerate(labels):\n",
    "        true_predictions.extend([predictions[i]] * len([l for l in label_list if l != -100]))\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, true_predictions, average='weighted', warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(true_labels, true_predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")  \n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        main_labels = []\n",
    "        for label_sequence in labels:\n",
    "            try:\n",
    "                main_label_index = next((i for i, label in enumerate(label_sequence) if label != -100), None)\n",
    "                if main_label_index is not None:\n",
    "                    main_labels.append(label_sequence[main_label_index])\n",
    "                else:\n",
    "                    main_labels.append(0)\n",
    "            except StopIteration:\n",
    "                main_labels.append(0)\n",
    "\n",
    "        main_labels = torch.tensor(main_labels, device=logits.device, dtype=torch.long)\n",
    "        loss = loss_fct(logits, main_labels)\n",
    "        return (loss, outputs) if return_outputs else loss \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Training Results:\", eval_results)\n",
    "\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(\"Validation Results:\", val_results)\n",
    "\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"Test Results:\", test_results)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
