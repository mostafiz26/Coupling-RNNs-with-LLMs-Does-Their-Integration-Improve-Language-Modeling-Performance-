{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeT5+-Base using code_x_glue_cc_defect_detection dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5EncoderModel, AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "# Dataset class\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, codes, labels, tokenizer, max_length):\n",
    "        self.codes = codes\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        code = self.codes[idx]\n",
    "        label = self.labels[idx]\n",
    "        encodings = self.tokenizer(code, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        input_ids = encodings['input_ids'].squeeze()\n",
    "        attention_mask = encodings['attention_mask'].squeeze()\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "# Model class\n",
    "class CodeClassifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes, hidden_dim):\n",
    "        super(CodeClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        pooled_output = encoder_outputs[:, 0]  # Use [CLS] token representation\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_code_x_glue_defect_detection():\n",
    "    dataset = load_dataset('code_x_glue_cc_defect_detection')\n",
    "    train_codes = dataset['train']['func']\n",
    "    train_labels = dataset['train']['target']\n",
    "    valid_codes = dataset['validation']['func']\n",
    "    valid_labels = dataset['validation']['target']\n",
    "    test_codes = dataset['test']['func']\n",
    "    test_labels = dataset['test']['target']\n",
    "    return (train_codes, train_labels), (valid_codes, valid_labels), (test_codes, test_labels)\n",
    "\n",
    "# Encode labels\n",
    "def encode_labels(labels):\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    return encoded_labels, encoder\n",
    "\n",
    "# Prepare DataLoader\n",
    "def create_dataloaders(train_data, valid_data, tokenizer, max_length, batch_size):\n",
    "    train_dataset = CodeDataset(train_data[0], train_data[1], tokenizer, max_length)\n",
    "    valid_dataset = CodeDataset(valid_data[0], valid_data[1], tokenizer, max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=5, learning_rate=2e-5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "\n",
    "    # Print total trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for input_ids, attention_mask, labels in train_loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        val_metrics = evaluate_model(model, val_loader, device, silent=True)\n",
    "        print(f'Validation Accuracy: {val_metrics[\"accuracy\"]:.4f}')\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, data_loader, device, silent=False):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, label in data_loader:\n",
    "            input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, label)\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(label.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "    if not silent:\n",
    "        print(\"Evaluation Metrics:\")\n",
    "        print(f'Loss: {avg_loss:.4f}')\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        print(f'Precision: {precision:.4f}')\n",
    "        print(f'Recall: {recall:.4f}')\n",
    "        print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "        # Generate classification report\n",
    "        report = classification_report(labels, preds, target_names=['True', 'False'])\n",
    "        print(\"Classification Report: \\n\", report)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        conf_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['True', 'False'], yticklabels=['True', 'False'])\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Save the model\n",
    "def save_model(model, path='code_classifier.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Load the model\n",
    "def load_model(path, encoder, num_classes):\n",
    "    model = CodeClassifier(encoder, num_classes, 512)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset\n",
    "    (train_codes, train_labels), (valid_codes, valid_labels), (test_codes, test_labels) = load_code_x_glue_defect_detection()\n",
    "\n",
    "    # Encode labels\n",
    "    train_labels, label_encoder = encode_labels(train_labels)\n",
    "    valid_labels = label_encoder.transform(valid_labels)\n",
    "    test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "    # Load tokenizer and preprocess data\n",
    "    pretrained_model_name = \"Salesforce/codet5p-220m\"  # Replace with your pre-trained model name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "    max_length = 512\n",
    "\n",
    "    # Prepare dataloaders\n",
    "    batch_size = 8\n",
    "    train_loader, val_loader = create_dataloaders((train_codes, train_labels), (valid_codes, valid_labels), tokenizer, max_length, batch_size)\n",
    "\n",
    "    # Load the encoder model\n",
    "    encoder_model = T5EncoderModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "    # Build and train the model\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CodeClassifier(encoder_model, num_classes, 512)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, device, num_epochs=5)\n",
    "\n",
    "    # Save the trained model\n",
    "    save_model(model, 'code_classifier.pth')\n",
    "\n",
    "    # Load the model for evaluation\n",
    "    loaded_model = load_model('code_classifier.pth', encoder_model, num_classes)\n",
    "    loaded_model.to(device)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Train Dataset:\")\n",
    "    train_metrics = evaluate_model(loaded_model, train_loader, device)\n",
    "    print(\"Validation Dataset:\")\n",
    "    val_metrics = evaluate_model(loaded_model, val_loader, device)\n",
    "\n",
    "    # Prepare test data loader\n",
    "    test_dataset = CodeDataset(test_codes, test_labels, tokenizer, max_length)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    print(\"Test Dataset:\")\n",
    "    test_metrics = evaluate_model(loaded_model, test_loader, device)\n",
    "\n",
    "    # Print metrics in desired format\n",
    "    print(\"\\neval_loss\\teval_accuracy\\teval_f1\\teval_precision\\teval_recall\")\n",
    "    print(f\"train\\t{train_metrics['loss']:.6f}\\t{train_metrics['accuracy']:.6f}\\t{train_metrics['f1']:.6f}\\t{train_metrics['precision']:.6f}\\t{train_metrics['recall']:.6f}\")\n",
    "    print(f\"val\\t{val_metrics['loss']:.6f}\\t{val_metrics['accuracy']:.6f}\\t{val_metrics['f1']:.6f}\\t{val_metrics['precision']:.6f}\\t{val_metrics['recall']:.6f}\")\n",
    "    print(f\"test\\t{test_metrics['loss']:.6f}\\t{test_metrics['accuracy']:.6f}\\t{test_metrics['f1']:.6f}\\t{test_metrics['precision']:.6f}\\t{test_metrics['recall']:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
