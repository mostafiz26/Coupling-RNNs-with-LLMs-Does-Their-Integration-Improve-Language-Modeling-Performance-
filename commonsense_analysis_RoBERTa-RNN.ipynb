{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RoBERTa-RNN model\n",
    "\n",
    "import torch, os\n",
    "import pandas as pd\n",
    "from transformers import pipeline, BertModel, BertTokenizer, BertForSequenceClassification,RobertaForSequenceClassification, RobertaTokenizer,T5ForConditionalGeneration,CodeLlamaTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device\n",
    "\n",
    "df_org= pd.read_csv(\"/content/drive/MyDrive/Dataset/processed_Tweets.csv\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "df_org = df_org.sample(frac=1.0, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "df_org.head()\n",
    "labels = df_org['airline_sentiment'].unique().tolist()\n",
    "labels\n",
    "NUM_LABELS= len(labels)\n",
    "\n",
    "id2label={id:label for id,label in enumerate(labels)}\n",
    "\n",
    "label2id={label:id for id,label in enumerate(labels)}\n",
    "\n",
    "label2id\n",
    "\n",
    "\n",
    "df_org[\"labels\"]=df_org.airline_sentiment.map(lambda x: label2id[x.strip()])\n",
    "df_org.head()\n",
    "df_org.airline_sentiment.value_counts().plot(kind='pie', figsize=(5,5))\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "fchidden = 256\n",
    "hiddendim_lstm = 128\n",
    "embeddim = 768\n",
    "numlayers = 5\n",
    "#checkpoint='google-bert/bert-base-cased'\n",
    "#checkpoint='microsoft/codebert-base'\n",
    "checkpoint='roberta-base'\n",
    "#checkpoint='Salesforce/codet5-small'\n",
    "#checkpoint='Salesforce/codet5p-220m'\n",
    "\n",
    "class Bert_LSTM(nn.Module):\n",
    "    def __init__(self, checkpoint, num_labels):\n",
    "        super(Bert_LSTM, self).__init__()\n",
    "        self.numclasses = num_labels\n",
    "        self.embeddim = embeddim\n",
    "        self.numlayers = numlayers\n",
    "        self.hiddendim_lstm = hiddendim_lstm\n",
    "\n",
    "        self.model= model = BertModel.from_pretrained(checkpoint, output_hidden_states=True, output_attentions=False)\n",
    "        print(\"BERT Model Loaded\")\n",
    "\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm = nn.LSTM(self.embeddim, self.hiddendim_lstm, batch_first=True, bidirectional=False) # noqa\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.embeddim, self.numclasses)\n",
    "        #self.classifier1 = nn.Linear(256, self.numclasses)\n",
    "\n",
    "    #def forward(self, inp_ids, att_mask, token_ids):\n",
    "    def forward(self, input_ids = None, attention_mask=None, labels = None ):\n",
    "\n",
    "        outputs = self.model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        sequence_outputs=outputs[0]\n",
    "\n",
    "        #sequence_outputs = self.dropout(sequence_outputs)\n",
    "        sequence_outputs = self.lstm(sequence_outputs)\n",
    "        #logits = self.classifier(sequence_outputs[:, 0, : ].view(-1, 768 ))\n",
    "        logits = self.classifier(sequence_outputs[:, -1])\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "            loss = loss_func(logits.view(-1, self.numclasses), labels.view(-1))\n",
    "\n",
    "            return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    " \"\"\"Bert Model for Classification Tasks.\"\"\"\n",
    " #def __init__(self, checkpoint, num_labels, freeze_bert=False):\n",
    " def __init__(self, checkpoint, num_labels):\n",
    "  super(BertClassifier, self).__init__()\n",
    "  self.numclasses = num_labels\n",
    "  self.embeddim = embeddim\n",
    "  self.numlayers = numlayers\n",
    "  self.hiddendim_lstm = hiddendim_lstm\n",
    "  # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "  #D_in, H, D_out = 768, 50, 2\n",
    "  # Instantiate BERT model\n",
    "  self.model= model = AutoModel.from_pretrained(checkpoint)\n",
    "  #self.model= model = RobertaForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "  self.dropout = nn.Dropout(0.2)\n",
    "  #self.activation=nn.ReLU()\n",
    "  self.lstm = nn.LSTM(self.embeddim, self.hiddendim_lstm, batch_first=True, bidirectional=False)\n",
    "  #self.dropout1 = nn.Dropout(0.1)\n",
    "  #self.linear = nn.Linear(self.hiddendim_lstm*2 , self.numclasses)False\n",
    "  self.linear = nn.Linear(self.hiddendim_lstm, self.numclasses)\n",
    "  self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  # Freeze the BERT model\n",
    "  #if freeze_bert:\n",
    "   #for param in self.model.parameters():\n",
    "    #param.requires_grad = False\n",
    "\n",
    " def forward(self, input_ids = None, attention_mask=None, labels = None ):\n",
    "  # Feed input to BERT\n",
    "  outputs = self.model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "\n",
    "  sequence_output = outputs[0]\n",
    "  #print(\"sequence_output size\", sequence_output.size())\n",
    "\n",
    "  sequence_output = self.dropout(sequence_output)\n",
    "  #sequence_output=self.activation(sequence_output)\n",
    "  sequence_output, _ = self.lstm(sequence_output)\n",
    "\n",
    "  #sequence_output = self.dropout1(sequence_output)\n",
    "\n",
    "  #print(\"lstm size\", sequence_output.size())\n",
    "\n",
    "  #sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "  logits = self.linear(sequence_output[:, -1])\n",
    "  logits = self.softmax(logits)\n",
    "\n",
    "  loss = None\n",
    "  loss = None\n",
    "  if labels is not None:\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loss = loss_func(logits.view(-1, self.numclasses), labels.view(-1))\n",
    "\n",
    "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(checkpoint, max_length=512)\n",
    "from transformers import RobertaTokenizer, BertForSequenceClassification\n",
    "\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(checkpoint, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)\n",
    "\n",
    "model=BertClassifier(checkpoint,NUM_LABELS)\n",
    "\n",
    "model.to(device)\n",
    "SIZE= df_org.shape[0]\n",
    "SIZE\n",
    "SIZE= df_org.shape[0]\n",
    "\n",
    "train_texts= list(df_org.CodeFilter[:(8*SIZE)//10])\n",
    "\n",
    "val_texts=   list(df_org.CodeFilter[(8*SIZE)//10:(90*SIZE)//100 ])\n",
    "\n",
    "test_texts=  list(df_org.CodeFilter[(90*SIZE)//100:])\n",
    "\n",
    "train_labels= list(df_org.labels[:(8*SIZE)//10])\n",
    "\n",
    "val_labels=   list(df_org.labels[(8*SIZE)//10:(90*SIZE)//100 ])\n",
    "\n",
    "test_labels=  list(df_org.labels[(90*SIZE)//100:])\n",
    "\n",
    "len(train_texts), len(val_texts), len(test_texts)\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings  = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "class DataLoader(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for handling tokenized text data and corresponding labels.\n",
    "    Inherits from torch.utils.data.Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Initializes the DataLoader class with encodings and labels.\n",
    "\n",
    "        Args:\n",
    "            encodings (dict): A dictionary containing tokenized input text data\n",
    "                              (e.g., 'input_ids', 'token_type_ids', 'attention_mask').\n",
    "            labels (list): A list of integer labels for the input text data.\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing tokenized data and the corresponding label for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the data item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            item (dict): A dictionary containing the tokenized data and the corresponding label.\n",
    "        \"\"\"\n",
    "        # Retrieve tokenized data for the given index\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Add the label for the given index to the item dictionary\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of data items in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            (int): The number of data items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "print(train_labels)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_encodings, train_labels)\n",
    "\n",
    "val_dataloader = DataLoader(val_encodings, val_labels)\n",
    "\n",
    "test_dataset = DataLoader(test_encodings, test_labels)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Computes accuracy, F1, precision, and recall for a given set of predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (obj): An object containing label_ids and predictions attributes.\n",
    "            - label_ids (array-like): A 1D array of true class labels.\n",
    "            - predictions (array-like): A 2D array where each row represents\n",
    "              an observation, and each column represents the probability of\n",
    "              that observation belonging to a certain class.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following metrics:\n",
    "            - Accuracy (float): The proportion of correctly classified instances.\n",
    "            - F1 (float): The macro F1 score, which is the harmonic mean of precision\n",
    "              and recall. Macro averaging calculates the metric independently for\n",
    "              each class and then takes the average.\n",
    "            - Precision (float): The macro precision, which is the number of true\n",
    "              positives divided by the sum of true positives and false positives.\n",
    "            - Recall (float): The macro recall, which is the number of true positives\n",
    "              divided by the sum of true positives and false negatives.\n",
    "    \"\"\"\n",
    "    # Extract true labels from the input object\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # Obtain predicted class labels by finding the column index with the maximum probability\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Compute macro precision, recall, and F1 score using sklearn's precision_recall_fscore_support function\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "\n",
    "    # Calculate the accuracy score using sklearn's accuracy_score function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    #mainreports=classification_report(preds, labels, target_names=['Insertion Sort', 'MST', 'Shell Sort', 'Exhaustive Search', 'BFS', 'SP', 'Linear Search', 'Selection Sort', 'Stable Sort', 'Binary Search', 'Rooted Trees', 'Bubble Sort', 'Counting Sort', 'Merge Sort', 'Projection', 'Convex Hull', 'NSS', 'Graph', 'Tree Walk', 'Puzzle', 'DFS', 'CBT', 'BST', 'Intersection', 'Binary Trees', 'Reflection', 'String Search', 'Quick Sort', 'Area'])\n",
    "    mainreports=classification_report(preds, labels, target_names=['negative', 'neutral', 'positive'])\n",
    "    #mainreports=classification_report(preds, labels, target_names=['DFS', 'Linear Search', 'Binary Search', 'Exhaustive Search', 'BFS'])\n",
    "\n",
    "    # Return the computed metrics as a dictionary\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'reports': mainreports\n",
    "    }\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "from transformers.optimization import AdamW, Adafactor, AdafactorSchedule\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "#optimizer = Adafactor(model.parameters(), relative_step=False, lr=1e-5, weight_decay=0.01)\n",
    "#lr_scheduler = AdafactorSchedule(optimizer)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.NAdam (model.parameters(), lr=2e-5)\n",
    "lr_scheduler=lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./imdbreviews_classification_roberta_v02\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    # the pre-trained model that will be fine-tuned\n",
    "    model=model,\n",
    "    # Optimizer\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "     # training arguments that we defined above\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataloader,\n",
    "    eval_dataset=val_dataloader,\n",
    "    compute_metrics= compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "q=[trainer.evaluate(eval_dataset=df_org) for df_org in [train_dataloader, val_dataloader, test_dataset]]\n",
    "\n",
    "pd.DataFrame(q, index=[\"train\",\"val\",\"test\"]).iloc[:,:5]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
